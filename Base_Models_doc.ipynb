{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/monk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# let's take one dataset for example\n",
    "from multilab.datasets import reuter\n",
    "sentences, labels = reuter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower_case done\n",
      "punctuation removed\n",
      "text cleaning done\n"
     ]
    }
   ],
   "source": [
    "# very few datapoints just to demonstrate how to use models\n",
    "\n",
    "# preprocessing\n",
    "from multilab.preprocess import Text_preprocessing\n",
    "text_preprocessing = Text_preprocessing()\n",
    "dataframe = text_preprocessing.labels_to_dataframe(sentences,labels)\n",
    "preprocessded_dataset = text_preprocessing.initial_preprocess(dataframe, chunk_value = 5)\n",
    "dataset_s , frequency_list_s = text_preprocessing.keep_labels(preprocessded_dataset,\n",
    "                                                           keep_ratio=0.1)\n",
    "\n",
    "# text preprocessing\n",
    "slice_dataset = text_preprocessing.dataset_slice(dataset_s,ratio=0.01)\n",
    "\n",
    "#tf-idf\n",
    "all_sentences, all_labels = text_preprocessing.tf_idf(slice_dataset)\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = text_preprocessing.split_dataset(all_sentences, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.4444444444444444, 'f1_score': 0.38383838383838387}\n"
     ]
    }
   ],
   "source": [
    "from multilab.models import BinaryRe\n",
    "Bm = BinaryRe(X_train, y_train, X_test,y_test)\n",
    "print(Bm.train())\n",
    "\n",
    "# four base models\n",
    "\n",
    "# BinaryRelevance\n",
    "# ClassifierChain\n",
    "# LabelPowerset\n",
    "# Mlknn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5185185185185185, 'f1_score': 0.5357142857142857}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from multilab.models import Classfierchains\n",
    "Cc = Classfierchains(X_train, y_train, X_test,y_test)\n",
    "print(Cc.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5185185185185185, 'f1_score': 0.5357142857142857}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/monk/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from multilab.models import labelpowerset\n",
    "lp = labelpowerset(X_train, y_train, X_test,y_test)\n",
    "print(lp.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eabb8202f196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhamming_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBilstm_simples\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBase_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hm'"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from hm import hamming_score\n",
    "from sklearn.metrics import f1_score\n",
    "from Bilstm_simples import Base_model\n",
    "\n",
    "\n",
    "import random\n",
    "import time\n",
    "import os \n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load configuation\n",
    "\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('config.properties')\n",
    "parameter_dict = dict(config.items('BiLstm network'))\n",
    "boolean_dict = {'None': None, 'True': True, 'False': False}\n",
    "\n",
    "#load data files \n",
    "with open(parameter_dict['sentence_path'] ,'rb') as f:\n",
    "    X_data = pk.load(f)\n",
    "\n",
    "with open(parameter_dict['labels_path']   ,'rb') as f:\n",
    "    y_data = pk.load(f)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, train_size=0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_data(batch_size, slice_no):\n",
    "\n",
    "\n",
    "    batch_data_j = np.array(X_train[slice_no * batch_size:(slice_no + 1) * batch_size])\n",
    "    batch_labels = np.array(y_train[slice_no * batch_size:(slice_no + 1) * batch_size])\n",
    "    \n",
    "    max_sequence = max(list(map(len, batch_data_j)))\n",
    "\n",
    "    # getting Max length of sequence\n",
    "    padded_sequence = [i + [0] * (max_sequence - len(i)) if len(i) < max_sequence else i for i in batch_data_j]\n",
    "    \n",
    "    return {'sentenc': padded_sequence, 'labels': batch_labels}\n",
    "\n",
    "\n",
    "def get_test_data(batch_size,slice_no):\n",
    "\n",
    "\n",
    "    batch_data_j = np.array(X_val[slice_no * batch_size:(slice_no + 1) * batch_size])\n",
    "    batch_labels = np.array(y_val[slice_no * batch_size:(slice_no + 1) * batch_size])\n",
    "    \n",
    "    max_sequence = max(list(map(len, batch_data_j)))\n",
    "    \n",
    "    padded_sequence = [i + [0] * (max_sequence - len(i)) if len(i) < max_sequence else i for i in batch_data_j]\n",
    "    \n",
    "    return {'sentenc': padded_sequence, 'labels': batch_labels}\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_(model, epoch_, batch_size = 120):\n",
    "\n",
    "    sess = tf.get_default_session()\n",
    "    iteration = len(X_val) // batch_size\n",
    "\n",
    "    sub_accuracy    = []\n",
    "    hamming_score_a = []\n",
    "    hamming_loss_   = []\n",
    "\n",
    "    micr_ac = []\n",
    "    weight_ac = []\n",
    "\n",
    "    for i in range(iteration):\n",
    "        \n",
    "        data_g = get_test_data(batch_size,i)\n",
    "        \n",
    "        sentences_data = data_g['sentenc']\n",
    "        labels_data    = data_g['labels']\n",
    "\n",
    "        network_out, targe = sess.run([model.predictions,model.targets], feed_dict={model.placeholders['sentence']: sentences_data,\n",
    "                                                                                    model.placeholders['labels']: labels_data, \n",
    "                                                                                    model.placeholders['dropout']: 0.0})\n",
    "\n",
    "        h_s     = hamming_score(targe, network_out)\n",
    "\n",
    "        ham_sco = h_s['hamming_score']\n",
    "        sub_acc = h_s['subset_accuracy']\n",
    "        ham_los = h_s['hamming_loss']\n",
    "\n",
    "        sub_accuracy.append(sub_acc)\n",
    "        hamming_score_a.append(ham_sco)\n",
    "        hamming_loss_.append(ham_los)\n",
    "\n",
    "\n",
    "\n",
    "        micr_ac.append(f1_score(targe, network_out, average='micro'))\n",
    "        weight_ac.append(f1_score(targe, network_out, average='weighted'))\n",
    "\n",
    "    return {  'subset_accuracy' : np.mean(np.array(sub_accuracy)) , \n",
    "              'hamming_score'   : np.mean(np.array(hamming_score_a)) , \n",
    "              'hamming_loss'    : np.mean(np.array(hamming_loss_)), \n",
    "               'micro_ac'       : np.mean(np.array(micr_ac)), \n",
    "               'weight_ac'      : np.mean(np.array(weight_ac)) , 'epoch': epoch_ }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, batch_size = int(parameter_dict['batch_size']), epoch = int(parameter_dict['epoch'])):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        iteration = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "        for i in range(epoch):\n",
    "            t = trange(iteration, desc='Bar desc', leave=True)\n",
    "\n",
    "            for j in t:\n",
    "\n",
    "\n",
    "\n",
    "                data_g = get_train_data(batch_size,j)\n",
    "                sentences_data = data_g['sentenc']\n",
    "                labels_data    = data_g['labels']\n",
    "\n",
    "\n",
    "\n",
    "                network_out, train, targe, losss  = sess.run([model.predictions, model.optimizer, model.targets,model.loss],\n",
    "                                          feed_dict={model.placeholders['sentence']: sentences_data,\n",
    "                                                     model.placeholders['labels']: labels_data,\n",
    "                                                     model.placeholders['dropout']: 0.2})\n",
    "\n",
    "                t.set_description(\"epoch {},  iteration {},  F1_score {},  loss {}\".format(i,\n",
    "                                                                                       j,\n",
    "                                                                                       f1_score(targe, \n",
    "                                                                                                network_out, \n",
    "                                                                                                average='micro'), \n",
    "                                                                                       losss))\n",
    "                t.refresh() # to show immediately the update\n",
    "\n",
    "\n",
    "            val_data = evaluate_(model, i, batch_size = 100)\n",
    "            print(\"validation_acc\",val_data)\n",
    "            with open('./result/iterres.txt', 'a') as f:\n",
    "                f.write(str({'test_accuracy':  val_data}) + '\\n')\n",
    "                \n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = Base_model(vocab_size                  =   int(parameter_dict['vocab_size']),\n",
    "                       rnn_units                   =   int(parameter_dict['rnn_units']), \n",
    "                       word_embedding_dim          =   int(parameter_dict['word_embedding_dim']),  \n",
    "                       no_of_labels                =   int(parameter_dict['no_of_labels']), \n",
    "                       learning_rate               =   float(parameter_dict['learning_rate']),   \n",
    "                       trained_embedding           =   boolean_dict[parameter_dict['trained_embedding']], \n",
    "                       train_embedding             =   boolean_dict[parameter_dict['train_embedding']],\n",
    "                       model_output                =   boolean_dict[parameter_dict['model_output']])\n",
    "    \n",
    "    train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bilstm(object):\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_test, y_test, configuration = None):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test  = X_test\n",
    "        self.y_test  = y_test\n",
    "        \n",
    "        configuration = {'vocab_size' : }\n",
    "        \n",
    "    def default_configuration(self):\n",
    "        \n",
    "        default_conf = {}\n",
    "        return default_conf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1,'b':2,'c':3}\n",
    "\n",
    "cus = None\n",
    "\n",
    "if cus:\n",
    "    a.update(cus)\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

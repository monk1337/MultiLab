{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/monk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "lower_case done\n",
      "punctuation removed\n",
      "text cleaning done\n"
     ]
    }
   ],
   "source": [
    "# let's take one dataset for example\n",
    "from multilab.datasets import reuter\n",
    "sentences, labels = reuter()\n",
    "\n",
    "from multilab.preprocess import Text_preprocessing\n",
    "text_preprocessing = Text_preprocessing()\n",
    "\n",
    "dataframe = text_preprocessing.labels_to_dataframe(sentences,labels)\n",
    "preprocessded_dataset = text_preprocessing.initial_preprocess(dataframe, chunk_value = 25)\n",
    "dataset, frequency_list = text_preprocessing.keep_labels(preprocessded_dataset,keep_ratio=0.10)\n",
    "slice_dataset = text_preprocessing.dataset_slice(dataset,ratio=0.25)\n",
    "\n",
    "import numpy as np\n",
    "all_sente = list(slice_dataset['text'])\n",
    "all_label = np.array(slice_dataset.drop('text', 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = text_preprocessing.split_dataset(all_sente, all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'tokens': Shape TensorShape([Dimension(None)]) is incompatible with TensorShape([Dimension(None), Dimension(None)])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6345f6fd87e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mel_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmo_Word_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mel_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/pro/MultiLab/multilab/Models/Bilstm_elmo/bilstm_elmo_word_train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m                        \u001b[0mtrain_elmo\u001b[0m                   \u001b[0;34m=\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_configuration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_elmo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                        \u001b[0moutput_type\u001b[0m                  \u001b[0;34m=\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mold_configuration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                         max_sentence_words          =   self.max_len)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pro/MultiLab/multilab/Models/Bilstm_elmo/bilstm_elmo_word.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, no_of_labels, learning_rate, rnn_units, train_elmo, output_type, max_sentence_words)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mmodule\u001b[0m                \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://tfhub.dev/google/elmo/2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_elmo\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         module_features       = module(dict(tokens=sentences, sequence_len = sequence_length),\n\u001b[0;32m---> 51\u001b[0;31m                                  signature='tokens', as_dict=True)\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0membeddings\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mmodule_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"elmo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, _sentinel, signature, as_dict)\u001b[0m\n\u001b[1;32m    248\u001b[0m     dict_inputs = _convert_dict_inputs(\n\u001b[1;32m    249\u001b[0m         inputs, self._spec.get_input_info_dict(signature=signature,\n\u001b[0;32m--> 250\u001b[0;31m                                                tags=self._tags))\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     dict_outputs = self._impl.create_apply_graph(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m_convert_dict_inputs\u001b[0;34m(inputs, tensor_info_map)\u001b[0m\n\u001b[1;32m    450\u001b[0m   \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_dict_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_info_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m   return tensor_info.convert_dict_to_compatible_tensor(dict_inputs,\n\u001b[0;32m--> 452\u001b[0;31m                                                        tensor_info_map)\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/tensor_info.py\u001b[0m in \u001b[0;36mconvert_dict_to_compatible_tensor\u001b[0;34m(values, targets)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     result[key] = _convert_to_compatible_tensor(\n\u001b[0;32m--> 150\u001b[0;31m         value, targets[key], error_prefix=\"Can't convert %r\" % key)\n\u001b[0m\u001b[1;32m    151\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_hub/tensor_info.py\u001b[0m in \u001b[0;36m_convert_to_compatible_tensor\u001b[0;34m(value, target, error_prefix)\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     raise TypeError(\"%s: Shape %r is incompatible with %r\" %\n\u001b[0;32m--> 129\u001b[0;31m                     (error_prefix, tensor.get_shape(), target.get_shape()))\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert 'tokens': Shape TensorShape([Dimension(None)]) is incompatible with TensorShape([Dimension(None), Dimension(None)])"
     ]
    }
   ],
   "source": [
    "from multilab.models import Elmo_Word_Model\n",
    "\n",
    "config = {\n",
    "                         'no_of_labels'               :  y_train.shape[1],\n",
    "                         'learning_rate'              : 0.001,\n",
    "                         'rnn_units'                  : 100,\n",
    "                         'epoch'                      : 1,\n",
    "                         'batch_size'                 : 128,\n",
    "                         'dropout'                    : 0.2,\n",
    "                         'output_type'                : 'state_output',\n",
    "                         'train_elmo'                 : True,\n",
    "                         'result_path'                : '/Users/monk/Desktop',\n",
    "                        }\n",
    "\n",
    "\n",
    "el_m = Elmo_Word_Model(X_train, y_train, X_test,  y_test, config)\n",
    "el_m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(el_m.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub\n",
    "\n",
    "ins = tf.placeholder(tf.string, (None), name='pl_phrase_text')\n",
    "\n",
    "\n",
    "module = hub.Module('https://tfhub.dev/google/elmo/2', trainable=True)\n",
    "embeddings = module(dict(text=ins))\n",
    "embeddings = tf.expand_dims(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    print(sess.run(embeddings,feed_dict={ins:['hello']}).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tokens = tf.placeholder(tf.string, (None), name='pl_phrase_text')\n",
    "pl_phrase_len = tf.placeholder(tf.int32, (None), name='pl_phrase_len')\n",
    "\n",
    "tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "                [\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
    "tokens_length = [6, 5]\n",
    "\n",
    "\n",
    "module = hub.Module('https://tfhub.dev/google/elmo/2', trainable=True)\n",
    "module_features = module(dict(tokens=tokens, sequence_len = pl_phrase_len),\n",
    "                                 signature='tokens', as_dict=True)\n",
    "embeddingse = module_features[\"elmo\"]\n",
    "\n",
    "with tf.variable_scope('Layer1'):\n",
    "    cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n",
    "    cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\n",
    "\n",
    "    outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=cell_fw1,\n",
    "        cell_bw=cell_bw1,\n",
    "        inputs=embeddingse,\n",
    "        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    out, data,t = sess.run([outputs1,embeddingse,module_features],feed_dict={tokens: tokens_input, pl_phrase_len: tokens_length})\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_output = tf.reshape(outputs1[0], (-1, 128))\n",
    "rnn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_outputs = tf.reshape(outputs1[0], (-1, 128 * 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# padded_sequence = [i + [0] * (max_sequence - len(i)) if len(i) < max_sequence else i for i in batch_data_j]\n",
    "\n",
    "\n",
    "sentences = ['check this out','i am very happy', 'what is', ['check','this'],'he a a a abs']\n",
    "\n",
    "\n",
    "def pad_sentences(self, sentences, sequence_len):\n",
    "    padded_sentences = []\n",
    "    actual_length    = []\n",
    "\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        if not isinstance(sentence, list):\n",
    "            token = nltk.word_tokenize(sentence)\n",
    "        else:\n",
    "            token = sentence\n",
    "        \n",
    "        if len(token) < sequene_len:\n",
    "            actual_length.append(len(token))\n",
    "            token = token + [''] * (sequene_len-len(token))\n",
    "        else:\n",
    "            actual_length.append(len(token))\n",
    "        \n",
    "        padded_sentences.append(token)\n",
    "    return padded_sentences, actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chy = pad_sentences(sentences,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_len = []\n",
    "for m in chy[0]:\n",
    "    actu = [k for k in m if k!='']\n",
    "    actual_len.append(len(actu))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def actual_len(list_data):\n",
    "    \n",
    "    actual_ = []\n",
    "    for sequence in list_data:\n",
    "        actual = [sub_ for sub_ in sequence if sub_!='']\n",
    "        actual_.append(len(actual))\n",
    "    return actual_\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data        = actual_len(chy[0])\n",
    "second_list = actual_len(chy[0])\n",
    "data.extend(second_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

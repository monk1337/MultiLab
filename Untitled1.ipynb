{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "try:\n",
    "    from tensorflow.python.util import module_wrapper as deprecation\n",
    "except ImportError:\n",
    "    from tensorflow.python.util import deprecation_wrapper as deprecation\n",
    "deprecation._PER_MODULE_WARNING_LIMIT = 0\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "with open('x_tr','rb') as f:\n",
    "    X_train = pk.load(f)\n",
    "    \n",
    "with open('x_te','rb') as f:\n",
    "    X_test = pk.load(f)\n",
    "    \n",
    "with open('y_tr','rb') as f:\n",
    "    y_train = pk.load(f)\n",
    "    \n",
    "with open('y_te','rb') as f:\n",
    "    y_test = pk.load(f)\n",
    "\n",
    "\n",
    "class Elmo_word_model(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                no_of_labels,\n",
    "                learning_rate, \n",
    "                rnn_units, \n",
    "                train_elmo = True, \n",
    "                output_type = 'state_output', \n",
    "                max_sentence_words = 150):\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # feature extraction network  ------------------------------------------>>\n",
    "        \n",
    "        # pass raw string \n",
    "        # one hot labels\n",
    "        sentences             = tf.placeholder(tf.string, (None), name='sentences')\n",
    "        self.targets          = tf.placeholder(tf.int32, [None, None], name='labels' )\n",
    "        sequence_length       = tf.placeholder(tf.int32, (None), name='sequence_len')\n",
    "\n",
    "        keep_prob             = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "\n",
    "\n",
    "        self.placeholders     = {\n",
    "                                'sentence': sentences, \n",
    "                                'labels': self.targets, \n",
    "                                'drop': keep_prob, \n",
    "                                'sequence_length': sequence_length\n",
    "                                }\n",
    "\n",
    "        module                = hub.Module('https://tfhub.dev/google/elmo/2', trainable = train_elmo )\n",
    "        module_features       = module(dict(tokens=sentences, sequence_len = sequence_length),\n",
    "                                 signature='tokens', as_dict=True)\n",
    "        embeddings            = module_features[\"elmo\"]\n",
    "\n",
    "\n",
    "\n",
    "        # sequence learning network -------------------------------------------------------->\n",
    "         #bilstm model\n",
    "        with tf.variable_scope('forward'):\n",
    "            fr_cell = tf.contrib.rnn.LSTMCell(num_units = rnn_units)\n",
    "            dropout_fr = tf.contrib.rnn.DropoutWrapper(fr_cell, output_keep_prob = 1. - keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('backward'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(num_units = rnn_units)\n",
    "            dropout_bw = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob = 1. - keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            model,last_state = tf.nn.bidirectional_dynamic_rnn(dropout_fr,\n",
    "                                                               dropout_bw,\n",
    "                                                               inputs = embeddings,\n",
    "                                                               dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        if output_type == 'flat':\n",
    "\n",
    "            logits = tf.reshape(model[0], (-1, rnn_units * max_sentence_words))\n",
    "            # dense layer with xavier weights\n",
    "            fc_layer = tf.get_variable(name='fully_connected',\n",
    "                                    shape=[rnn_units * max_sentence_words, no_of_labels],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # bias \n",
    "            bias    = tf.get_variable(name='bias',\n",
    "                                    shape=[no_of_labels],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #final output \n",
    "            self.x_ = tf.add(tf.matmul(logits,fc_layer),bias)\n",
    "\n",
    "        else:\n",
    "\n",
    "            logits = tf.concat([last_state[0].c,last_state[1].c],axis=-1)\n",
    "             # dense layer with xavier weights\n",
    "            fc_layer = tf.get_variable(name='fully_connected',\n",
    "                                    shape=[2*rnn_units, no_of_labels],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            # bias \n",
    "            bias    = tf.get_variable(name='bias',\n",
    "                                    shape=[no_of_labels],\n",
    "                                    dtype=tf.float32,\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            #final output \n",
    "            self.x_ = tf.add(tf.matmul(logits,fc_layer),bias)\n",
    "\n",
    "\n",
    "         #optimization and loss calculation ---------------------------------->>\n",
    "        \n",
    "        self.cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = self.x_, labels = tf.cast(self.targets,tf.float32))\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(self.cross_entropy, axis=1))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.loss)\n",
    "        self.predictions = tf.cast(tf.sigmoid(self.x_) > 0.5, tf.int32)\n",
    "        \n",
    "# # let's take one dataset for example\n",
    "# import pickle as pk\n",
    "# from multilab.datasets import reuter\n",
    "# sentences, labels = reuter()\n",
    "\n",
    "# from multilab.preprocess import Text_preprocessing\n",
    "# text_preprocessing = Text_preprocessing()\n",
    "\n",
    "# dataframe = text_preprocessing.labels_to_dataframe(sentences,labels)\n",
    "# preprocessded_dataset = text_preprocessing.initial_preprocess(dataframe, chunk_value = 25)\n",
    "# dataset, frequency_list = text_preprocessing.keep_labels(preprocessded_dataset,keep_ratio=0.10)\n",
    "# slice_dataset = text_preprocessing.dataset_slice(dataset,ratio=0.25)\n",
    "\n",
    "# import numpy as np\n",
    "# all_sente = list(slice_dataset['text'])\n",
    "# all_label = np.array(slice_dataset.drop('text', 1))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = text_preprocessing.split_dataset(all_sente, all_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "def pad_sentences(sentences):\n",
    "        \n",
    "\n",
    "    padded_sentences = []\n",
    "    actual_length    = []\n",
    "\n",
    "    sentences = [seq.split() for seq in sentences]\n",
    "    sequence_len = max(list(map(len, sentences)))\n",
    "\n",
    "\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        if not isinstance(sentence, list):\n",
    "            token = nltk.word_tokenize(sentence)\n",
    "        else:\n",
    "            token = sentence\n",
    "\n",
    "        if len(token) < sequence_len:\n",
    "            actual_length.append(len(token))\n",
    "            token = token + [''] * (sequence_len-len(token))\n",
    "        else:\n",
    "            actual_length.append(len(token))\n",
    "\n",
    "        padded_sentences.append(token)\n",
    "    return padded_sentences, actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_len(padded_list):\n",
    "\n",
    "    actual_ = []\n",
    "    for sequence in padded_list:\n",
    "        actual = [sub_ for sub_ in sequence if sub_!='']\n",
    "        actual_.append(len(actual))\n",
    "    return actual_\n",
    "\n",
    "def max_length(sequences):\n",
    "    \n",
    "    actual_ = [len(sequence.split()) for sequence in sequences]\n",
    "    return max(actual_)\n",
    "\n",
    "\n",
    "# X_train,_ = pad_sentences(X_train)\n",
    "# y_train = y_train\n",
    "# X_val,_   = pad_sentences(X_test)\n",
    "# y_val   = y_test\n",
    "\n",
    "\n",
    "# train_len    = actual_len(X_train)\n",
    "# test_len     = actual_len(X_val)\n",
    "# train_len.extend(test_len)\n",
    "\n",
    "# max_len = max(train_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train   = X_train\n",
    "y_train   = y_train\n",
    "X_val    = X_test\n",
    "y_val   = y_test\n",
    "max_len   = max_length(X_train)\n",
    "\n",
    "old_configuration = {}\n",
    "\n",
    "config = {\n",
    "                         'no_of_labels'               :  y_train.shape[1],\n",
    "                         'learning_rate'              : 0.001,\n",
    "                         'rnn_units'                  : 100,\n",
    "                         'epoch'                      : 1,\n",
    "                         'batch_size'                 : 12,\n",
    "                         'dropout'                    : 0.2,\n",
    "                         'output_type'                : 'state_output',\n",
    "                         'train_elmo'                 : True,\n",
    "                         'result_path'                : '/Users/monk/Desktop',\n",
    "                        }\n",
    "\n",
    "old_configuration.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(batch_size, slice_no):\n",
    "\n",
    "\n",
    "    batch_data_j = X_train[slice_no * batch_size:(slice_no + 1) * batch_size]\n",
    "    batch_labels = y_train[slice_no * batch_size:(slice_no + 1) * batch_size]\n",
    "    batch_data_j,lens  = pad_sentences(batch_data_j)\n",
    "\n",
    "\n",
    "    return {'sentenc': np.array(batch_data_j), 'labels': np.array(batch_labels) ,'sequence_len': lens}\n",
    "    \n",
    "    \n",
    "    # test data loader\n",
    "def get_test_data(self, batch_size,slice_no):\n",
    "\n",
    "\n",
    "    batch_data_j = X_val[slice_no * batch_size:(slice_no + 1) * batch_size]\n",
    "    batch_labels = y_val[slice_no * batch_size:(slice_no + 1) * batch_size]\n",
    "    batch_data_j,lens  = pad_sentences(batch_data_j)\n",
    "\n",
    "\n",
    "    return {'sentenc': np.array(batch_data_j), 'labels': np.array(batch_labels) ,'sequence_len': lens}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10425.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentenc': array([['argentine', 'port', 'workers', 'take', 'industrial', 'action',\n",
       "         'argentine', 'port', 'workers', 'began', 'an', 'indefinite',\n",
       "         'protest', 'against', 'safety', 'conditions', 'at', 'the',\n",
       "         'port', 'of', 'buenos', 'aires', 'stopping', 'work', 'for'],\n",
       "        ['green', 'tree', 'acceptance', 'inc', 'ltgnt', 'sets',\n",
       "         'dividend', 'qtly', 'dividend', 'cts', 'vs', 'cts', 'pay',\n",
       "         'march', 'record', 'march', '', '', '', '', '', '', '', '', ''],\n",
       "        ['time', 'lttl', 'to', 'sell', 'part', 'of', 'unit', 'time',\n",
       "         'inc', 'said', 'its', 'timelife', 'video', 'inc', 'subsidiary',\n",
       "         'has', 'agreed', 'in', 'principle', 'to', 'sell', 'its',\n",
       "         'institutional', 'training', 'business'],\n",
       "        ['gulf', 'barge', 'freight', 'higher', 'in', 'nearbys', 'on',\n",
       "         'call', 'gulf', 'barge', 'freight', 'rates', 'continued', 'to',\n",
       "         'show', 'a', 'firmer', 'tone', 'in', 'the', 'nearbys', 'on',\n",
       "         'the', 'assumption', 'that'],\n",
       "        ['multibank', 'financial', 'corp', 'ltmltfo', 'rd', 'qtr', 'net',\n",
       "         'shr', 'cts', 'vs', 'cts', 'net', 'vs', 'avg', 'shrs', 'vs',\n",
       "         'nine', 'mths', 'shr', 'dlrs', 'vs', 'dlrs', 'net', 'mln', 'vs'],\n",
       "        ['agency', 'reports', 'ships', 'waiting', 'at', 'panama', 'canal',\n",
       "         'the', 'panama', 'canal', 'commission', 'a', 'us', 'government',\n",
       "         'agency', 'said', 'in', 'its', 'daily', 'operations', 'report',\n",
       "         'that', 'there', 'was', 'a'],\n",
       "        ['honeywell', 'bull', 'sees', 'revenue', 'growth', 'honeywell',\n",
       "         'bull', 'inc', 'owned', 'by', 'honeywell', 'inc', 'lthon',\n",
       "         'ltcie', 'des', 'machines', 'bull', 'and', 'ltnec', 'corp',\n",
       "         'said', 'it', 'expects', 'its', 'revenues'],\n",
       "        ['hong', 'kong', 'february', 'trade', 'swings', 'into', 'deficit',\n",
       "         'hong', 'kong', 'recorded', 'a', 'billion', 'hk', 'dlr',\n",
       "         'deficit', 'in', 'february', 'after', 'a', 'billion', 'dlr',\n",
       "         'surplus', 'in', 'january', 'as'],\n",
       "        ['drexel', 'official', 'has', 'stake', 'in', 'epsilon', 'data',\n",
       "         'ltepsi', 'a', 'senior', 'official', 'of', 'drexel', 'burnham',\n",
       "         'lambert', 'inc', 'and', 'his', 'father', 'told', 'the',\n",
       "         'securities', 'and', 'exchange', 'commission'],\n",
       "        ['bank', 'of', 'japan', 'intervenes', 'to', 'buy', 'dollars',\n",
       "         'around', 'yen', 'dealers', 'bank', 'of', 'japan', 'intervenes',\n",
       "         'to', 'buy', 'dollars', 'around', 'yen', 'dealers', '', '', '',\n",
       "         '', '']], dtype='<U13'), 'labels': array([[0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0]]), 'sequence_len': [25,\n",
       "  16,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  25,\n",
       "  20]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/monk/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# let's take one dataset for example\n",
    "from multilab.datasets import reuter\n",
    "sentences, labels = reuter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing preprocessing class\n",
    "# let's play with that\n",
    "\n",
    "from multilab.preprocess import Text_preprocessing\n",
    "text_preprocessing = Text_preprocessing()\n",
    "dataframe = text_preprocessing.labels_to_dataframe(sentences,labels)\n",
    "preprocessded_dataset = text_preprocessing.initial_preprocess(dataframe, chunk_value = 150)\n",
    "\n",
    "dataset_s , frequency_list_s = text_preprocessing.keep_labels(preprocessded_dataset,\n",
    "                                                           keep_ratio=0.10)\n",
    "\n",
    "slice_dataset = text_preprocessing.dataset_slice(dataset_s,ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't want to use full data then sample the data\n",
    "\n",
    "# options : \n",
    "# frac = % slice (ex 25% data)\n",
    "# value = value slice (ex 1200 samples only )\n",
    "\n",
    "slice_dataset = text_preprocessing.dataset_slice(dataset_s,ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:00<00:00, 69317.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the vocabulary frequency and then decide how many words you want to keep in vocab\n",
    "\n",
    "# options :\n",
    "# keep ration  = keep top 25%, 50%, 75% or any % vocab\n",
    "# freq_value   = keep only those words whose frequency is >= freq_value\n",
    "# custom_value = keep top n ( ex 1400 )  words \n",
    "\n",
    "# first checking the vocab freq : all arguments are false\n",
    "sorted_long, freq_num, word_to_int, int_to_word  = text_preprocessing.vocab_freq(slice_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9674), ('of', 5669), ('to', 5203), ('in', 4128), ('mln', 3967), ('said', 3956), ('and', 3884), ('a', 3774), ('vs', 3734), ('dlrs', 2399)]\n",
      "13777\n"
     ]
    }
   ],
   "source": [
    "print(sorted_long[:10])\n",
    "print(len(sorted_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:00<00:00, 63894.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# or use top 25% words\n",
    "top_words, freq_lis, w2i, i2w = text_preprocessing.vocab_freq(slice_dataset,keep_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:00<00:00, 85572.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# or use only those words whose frequency is more than 12\n",
    "freq_words, fre_li , w2_i, i_2w = text_preprocessing.vocab_freq(slice_dataset,freq_Value=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735\n",
      "[('a', 3774), ('ab', 12), ('able', 19), ('about', 424), ('above', 51), ('accept', 13), ('accepted', 25), ('accord', 65), ('according', 51), ('account', 59)]\n"
     ]
    }
   ],
   "source": [
    "print(len(freq_words))\n",
    "print(freq_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2491/2491 [00:01<00:00, 1894.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's encode the sentences \n",
    "\n",
    "all_sentence_s, all_label_s, vocab_dict = text_preprocessing.encoder(slice_dataset,w2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2491 2491 1737\n",
      "[582, 398, 1707, 1, 1717, 953, 456, 1562, 582, 680, 715, 309, 1, 1, 1, 1704, 1107, 953, 456, 611, 1562, 530, 352, 1, 1, 217, 1026, 1, 775, 744, 1623, 1562, 52, 1252, 743, 1178, 176, 1562, 1618, 308, 2, 1477, 224, 582, 1280, 224, 805, 1, 1032, 1348, 1695, 1, 83, 1562, 52, 1728, 743, 997, 1734, 800, 1348, 1045, 1561, 1562, 1170, 1718, 156, 743, 534, 1026, 953, 456, 582, 1348, 1562, 398, 796, 1501, 1582, 52, 1043, 1397, 1, 1026, 1562, 342, 82, 991, 669, 1, 800, 1348, 1562, 398, 1718, 1, 1, 1, 1, 5, 1115, 743, 501, 530, 307, 362, 1138, 563, 82, 299, 1056, 743, 1070, 487, 1479, 1562, 1, 697, 2, 239, 1582, 1187, 1562, 524, 1026, 953, 1591, 1026, 1, 743, 1, 82, 1, 2, 1723, 1063, 5, 1044, 1570, 1026, 487, 1193, 631, 5, 953, 1591, 1026, 257, 582, 1348]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_sentences),len(all_sentences),len(vocab_dict))\n",
    "print(all_sentences[1])\n",
    "print(all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use tf-idf\n",
    "\n",
    "all_sentences, all_labels = text_preprocessing.tf_idf(slice_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2491x234879 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 479283 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1737\n",
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1737/1737 [00:00<00:00, 232800.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vocab embedding\n",
    "\n",
    "path_of_glove_embedding = 'glove.6B.300d.txt'\n",
    "\n",
    "# out of vocab words are encoded with 'unk' vectors\n",
    "print(len(vocab_dict))\n",
    "vocab, out_of_vocab_words = text_preprocessing.vocab_embedding(vocab_dict, path_of_glove_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1737, 300)\n",
      "['carryforward', 'carryforwards', 'chemlawn', 'dlrsbbl', 'dlrsshr', 'fiveyear', 'incs', 'ltxon', 'mths', 'qtly', 'qtrly', 'shortterm', 'shrs', 'sosnoff', 'stateowned', 'taxfree', 'threefortwo', 'twoforone', 'whollyowned', 'yearago']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.shape)\n",
    "print(out_of_vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "# options : test value : defalt : 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = text_preprocessing.split_dataset(all_sentence_s, all_label_s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1743 748 1743 748\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test),len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
